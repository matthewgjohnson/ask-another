model|context|tok_sec|price_in|price_out|aa_index|arena_elo|swe_bench|gpqa|favourite|description
gemini/gemini-3.1-pro-preview|1000000|91|2.00|12.00|57|1500|80.6|94.3|yes|Google's frontier reasoning model, AA #1, ARC-AGI-2 77.1%, best price-to-quality at the frontier, 1M context, multimodal-native
openai/gpt-5.2|200000|98|1.75|14.00|51|1486|80.0|92.4|yes|OpenAI current-gen workhorse, 100% AIME (Pro), fastest frontier at 98 tok/s, RLHF-heavy assertive reasoning style
openrouter/z-ai/glm-5|128000|70|0.80|2.56|50|1451|77.8|86.0|yes|#1 open-weight, 745B MoE MIT license, lowest hallucination, trained on Huawei Ascend, maximally independent
openrouter/deepseek/deepseek-v3.2|128000|39|0.028|0.42|66|1480|74.0|81.0|yes|685B MoE MIT, gold IMO 2025, transparent RL reasoning chains, absurd value at 1/50th frontier cost
openrouter/moonshotai/kimi-k2.5|131072|42|0.60|3.00|47|1309|76.8|87.6|yes|1T MoE with 100-agent swarm, structurally novel parallel decomposition, vision-to-code, open-weight
openai/gpt-5.3-codex|200000|99|1.75|14.00|54||77.3||no|AA #2, Terminal-Bench SOTA 77.3%, fastest frontier, Spark variant 1000+ tok/s, dedicated coding
openai/gpt-5.2-pro|200000|98|21.00|84.00||1534||93.2|no|100% AIME 2025, highest GPQA 93.2%, 12x cost of standard GPT-5.2, maximum reasoning depth
openrouter/anthropic/claude-opus-4.6|200000|72|5.00|25.00|53|1503|80.8|91.3|no|Arena #1 (1503 Elo), best coding and creative writing by human preference, 1M context beta
openrouter/anthropic/claude-sonnet-4.6|200000|80|1.50|7.50||1475||88.0|no|70% preferred over Sonnet 4.5, 1M context beta, strong coding, best mid-range Anthropic
openrouter/minimax/minimax-m2.5|1000000|100|0.15|1.20|42||80.2|62.0|no|229B MoE open-weights, SWE-bench 80.2% at 1/20th Opus cost, Lightning 100 tok/s, narrow GPQA suggests coding-optimised
openrouter/qwen/qwen3.5-397b-a17b|131072|28|0.40|1.20|45||76.4|88.4|no|397B MoE (17B active), Apache 2.0, 91.3% AIME 2026, 201 languages, correlated with DeepSeek approach
openrouter/bytedance-seed/seed-2.0-mini|256000||0.10|0.40||1470|67.9||no|ByteDance cost leader, 256K context, four reasoning modes, 62% hallucination accuracy limits thinking partnership
openrouter/baidu/ernie-4.5-300b-a47b|128000|32|0.84|3.37||1460||57.0|no|Baidu 2.4T ultra-sparse MoE, Arena #8 globally (Ernie 5.0), strong multimodal, instruction-following inconsistencies
openrouter/deepseek/deepseek-r1-0528|128000|35|0.55|2.19||1444|87.5||no|Best open-source reasoning, MIT, 87.5% AIME, transparent chain-of-thought, superseded by V3.2 for general use
openrouter/z-ai/glm-4.7|128000|85|0.10|0.10||1445||95.7|no|GLM-5's cheaper sibling, Arena 1445, 95.7% AIME, essentially free frontier-adjacent coding
openrouter/meta-llama/llama-4-maverick|128000|75|0.20|0.60||||no|Meta open MoE, strong multilingual, free on OpenRouter, battle-tested ecosystem
openrouter/meta-llama/llama-4-scout|10000000|60|0.15|0.40|||||no|10M token context (largest available), smaller model, unique for extreme context use cases
openai/gpt-oss-120b|128000|55|0.00|0.00|||80.9||no|OpenAI first open-weight, MMLU 90.0, GPQA 80.9%, free on OpenRouter, strategic signal
openrouter/google/gemma-3-27b-it|128000|90|0.00|0.00|||||no|Google open model, free on OpenRouter, good for basic tasks and fine-tuning
